{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io as sio\n",
    "import scipy.signal as sig\n",
    "import pywt\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "import tensorflow.contrib.rnn as recurrent\n",
    "import sklearn.preprocessing\n",
    "#\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Load utility codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from codes.pre_processing import *\n",
    "from codes.segmentation import *\n",
    "from codes.utils import *\n",
    "from codes.training import *\n",
    "from codes.model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training/Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_root_dir = '../training_set'\n",
    "train_file_list, val_file_list = test_val_split_v2(data_root_dir, train_percentage = 90)\n",
    "ref_file = os.path.join(data_root_dir, 'REFERENCE.csv')\n",
    "#\n",
    "list_of_training_files = np.array(train_file_list)\n",
    "list_of_validation_files = np.array(val_file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1000 x 12 batches\n",
    "input_size = 12\n",
    "time_steps = 1000\n",
    "num_classes = 9\n",
    "hidden_size = 100\n",
    "num_hidden = 2\n",
    "output_size = 9\n",
    "keep_prob = 0.5  # dropout\n",
    "inputs = tf.placeholder(tf.float32, [None, time_steps, input_size])\n",
    "labels = tf.placeholder(tf.int32, [None])\n",
    "seq_length = tf.placeholder(tf.int32, [None])\n",
    "#\n",
    "is_training = True # set it to true at first\n",
    "#\n",
    "def RNN_bidirectional(input_tensor, Training):\n",
    "    with tf.variable_scope(\"recurrent\", initializer = tf.contrib.layers.variance_scaling_initializer()):\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell\n",
    "        cells_fw = [cell(hidden_size) for _ in range(num_hidden)]\n",
    "        cells_bw = [cell(hidden_size) for _ in range(num_hidden)]\n",
    "        cells_fw = [tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob if Training is True else 1.0) for cell in cells_fw]\n",
    "        cells_bw = [tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob if Training is True else 1.0) for cell in cells_bw]\n",
    "        _, states_fw, states_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "                cells_fw=cells_fw,\n",
    "                cells_bw=cells_bw,\n",
    "                inputs=input_tensor,\n",
    "                sequence_length = seq_length,\n",
    "                dtype=tf.float32)\n",
    "        outputs_fw = tf.concat(states_fw[-1][-1], axis = 1)\n",
    "        outputs_bw = tf.concat(states_bw[-1][-1], axis = 1)\n",
    "        outputs = tf.concat([outputs_fw, outputs_bw], axis = 1)\n",
    "        logits = tf.squeeze(fully_connected(outputs, output_size, activation_fn = None))\n",
    "        #\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logits = RNN_bidirectional(inputs, Training = is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss and training ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = labels, logits = logits)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "#\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "# gradient clipping - makes the training more stable\n",
    "gradients = [\n",
    "    None if gradient is None else tf.clip_by_norm(gradient, 5.0)\n",
    "    for gradient in gradients]\n",
    "training_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "#\n",
    "correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep = 5, keep_checkpoint_every_n_hours = 1)\n",
    "save_dir = './model'\n",
    "#\n",
    "model_name_prefix = 'model.ckpt'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session()  as sess:\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    path = tf.train.get_checkpoint_state(save_dir)\n",
    "    if path is None:\n",
    "        global_step = 0\n",
    "    else:\n",
    "        global_step = int(path.model_checkpoint_path.split('-')[-1])\n",
    "    #\n",
    "    \n",
    "    if path is None:\n",
    "        sess.run(init_op)\n",
    "    else:\n",
    "        saver.restore(sess, path.model_checkpoint_path)\n",
    "    \n",
    "    for step in range(20000):\n",
    "        #\n",
    "        data_numpy, labels_numpy, seq_length_numpy = sample_batch_for_training(list_of_training_files, ref_file)\n",
    "        _, loss_value, logits_value = sess.run([training_op, loss, logits], feed_dict = {inputs: data_numpy, labels: labels_numpy, seq_length: seq_length_numpy})\n",
    "        #\n",
    "        if step % 20 == 0:\n",
    "            print('current iteration: {}'.format(step + global_step))\n",
    "            print('loss value: {}'.format(loss_value))\n",
    "            acc_train = sess.run(accuracy, feed_dict = {inputs: data_numpy, labels: labels_numpy, seq_length: seq_length_numpy})\n",
    "            is_training = False  # set to false before evaluating the test accuracy\n",
    "            sub = np.random.randint(len(list_of_validation_files))\n",
    "            data_val, labels_val, seq_length_val = sample_batch(list_of_validation_files[sub], ref_file, mode = 'training')\n",
    "            acc_val = sess.run(accuracy, feed_dict = {inputs: data_val, labels: labels_val, seq_length: seq_length_val})\n",
    "            is_training = True  # set to true again for the subsequent iterations\n",
    "            #\n",
    "            print(step, \"Training accuracy:\", acc_train, \"Test accuracy\", acc_val)\n",
    "            try:\n",
    "                loss_list.append(loss_value)\n",
    "                acc_train_list.append(acc_train)\n",
    "                acc_test_list.append(acc_val)\n",
    "            except:\n",
    "                loss_list = list()\n",
    "                acc_train_list = list()\n",
    "                acc_test_list = list()\n",
    "                loss_list.append(loss_value)\n",
    "                acc_train_list.append(acc_train)\n",
    "                acc_test_list.append(acc_val)\n",
    "        save_path = saver.save(sess, os.path.join(save_dir, model_name_prefix), global_step = step + global_step + 1) # model count begins with 1\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "        \n",
    "        if loss_value < 0.10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(num = 1)\n",
    "plt.plot(range(0, step, 20), acc_train_list, 'k', range(0, step, 20), acc_test_list, 'b')\n",
    "plt.figure(num = 2)\n",
    "plt.plot(range(0, step, 20), loss_list, 'r')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
